
//Week-9
//Logistic Regression 

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator

val evaluator = new MulticlassClassificationEvaluator()
  .setLabelCol("label")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")

val accuracy = evaluator.evaluate(predictions)
println(s"Test set accuracy = $accuracy")

//___________________________________________
//Decision Tree Classifier 

// Import Libraries
import org.apache.spark.ml.classification.DecisionTreeClassifier
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.sql.SparkSession

// Create Spark Session
val spark = SparkSession.builder().appName("Decision Tree Classifier").getOrCreate()

// Load Data
val data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")

// Split Data into Training and Testing Sets
val Array(trainingData, testData) = data.randomSplit(Array(0.8, 0.2))

// Initialize Decision Tree Classifier
val dt = new DecisionTreeClassifier()
  .setLabelCol("label")
  .setFeaturesCol("features")

// Train the Model
val model = dt.fit(trainingData)

// Make Predictions
val predictions = model.transform(testData)

// Evaluate the Model
val evaluator = new MulticlassClassificationEvaluator()
  .setLabelCol("label")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")

val accuracy = evaluator.evaluate(predictions)
println(s"Test set accuracy = $accuracy")

// Print the Decision Tree Model
println(s"Learned classification tree model:\n ${model.toDebugString}")

//_________________________________________________
//Naive Bayes 
// Import Libraries
import org.apache.spark.ml.classification.NaiveBayes
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.sql.SparkSession

// Create Spark Session
val spark = SparkSession.builder().appName("Naive Bayes Classifier").getOrCreate()

// Load Data
val data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")

// Split Data into Training and Testing Sets
val Array(trainingData, testData) = data.randomSplit(Array(0.8, 0.2))

// Initialize Naive Bayes Classifier
val nb = new NaiveBayes()
  .setLabelCol("label")
  .setFeaturesCol("features")
  .setModelType("multinomial") // Options: "multinomial" or "bernoulli"

// Train the Model
val model = nb.fit(trainingData)

// Make Predictions
val predictions = model.transform(testData)

// Evaluate the Model
val evaluator = new MulticlassClassificationEvaluator()
  .setLabelCol("label")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")

val accuracy = evaluator.evaluate(predictions)
println(s"Test set accuracy = $accuracy")
//______________________________________
//Week-10
// Import Libraries
import org.apache.spark.ml.clustering.KMeans
import org.apache.spark.ml.evaluation.ClusteringEvaluator
import org.apache.spark.sql.SparkSession

// Create Spark Session
val spark = SparkSession.builder().appName("K-Means Clustering").getOrCreate()

// Load Data
val data = spark.read.format("libsvm").load("data/mllib/sample_kmeans_data.txt")

// Initialize K-Means Clustering
val kmeans = new KMeans()
  .setK(3)                // Number of clusters
  .setSeed(1L)            // Random seed
  .setFeaturesCol("features")

// Train the Model
val model = kmeans.fit(data)

// Make Predictions
val predictions = model.transform(data)

// Evaluate the Model
val evaluator = new ClusteringEvaluator()

val silhouette = evaluator.evaluate(predictions)
println(s"Silhouette with squared euclidean distance = $silhouette")

// Print Cluster Centers
println("Cluster Centers: ")
model.clusterCenters.foreach(println)

//_______________________________________
//Latent Dirichlet Allocation // Import Libraries
import org.apache.spark.ml.clustering.LDA
import org.apache.spark.sql.SparkSession

// Create Spark Session
val spark = SparkSession.builder().appName("Latent Dirichlet Allocation").getOrCreate()

// Load Data
val data = spark.read.format("libsvm").load("data/mllib/sample_lda_libsvm_data.txt")

// Initialize LDA
val lda = new LDA()
  .setK(3)                // Number of topics
  .setMaxIter(10)         // Number of iterations
  .setFeaturesCol("features")

// Train the Model
val model = lda.fit(data)

// Describe Topics
val topics = model.describeTopics(5) // Get top 5 terms for each topic
topics.show(false)

// Transform Data
val transformed = model.transform(data)
transformed.show(false)
//_________________________________________________
//Gaussian Mixture Model (GMM)
// Import Libraries
import org.apache.spark.ml.clustering.GaussianMixture
import org.apache.spark.sql.SparkSession

// Create Spark Session
val spark = SparkSession.builder().appName("Gaussian Mixture Model").getOrCreate()

// Load Data
val data = spark.read.format("libsvm").load("data/mllib/sample_kmeans_data.txt")

// Initialize Gaussian Mixture Model
val gmm = new GaussianMixture()
  .setK(2)          // Number of clusters
  .setFeaturesCol("features")
  .setSeed(1L)      // Random seed for reproducibility

// Train the Model
val model = gmm.fit(data)

// Make Predictions
val predictions = model.transform(data)
predictions.show(false)

// Print Parameters of the Gaussian Components
println("Gaussian Components:")
model.gaussians.zipWithIndex.foreach { case (g, idx) =>
  println(s"Cluster $idx:")
  println(s"  Mean: ${g.mean}")
  println(s"  Covariance: ${g.cov}")
}

//____________________________________
//week-11
// Import Libraries
import org.apache.spark.mllib.recommendation.{ALS, Rating}
import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.evaluation.RegressionMetrics

// Create Spark Session (Not needed in Spark 1.6.0 shell, but for script completeness)
val spark = SparkSession.builder().appName("Collaborative Filtering").getOrCreate()

// Create Synthetic Data (User, Movie, Rating) - Embedding dataset directly in code
val ratingsData = Seq(
  Rating(0, 0, 5.0),
  Rating(0, 1, 3.0),
  Rating(0, 2, 4.0),
  Rating(1, 0, 3.0),
  Rating(1, 1, 5.0),
  Rating(1, 2, 1.0),
  Rating(2, 0, 2.0),
  Rating(2, 1, 4.0),
  Rating(2, 2, 5.0),
  Rating(3, 0, 4.0),
  Rating(3, 1, 4.0),
  Rating(3, 2, 2.0)
)

// Convert to RDD
val ratingsRDD: RDD[Rating] = sc.parallelize(ratingsData)

// Split Data into Training and Testing Sets (80% train, 20% test)
val Array(trainingData, testData) = ratingsRDD.randomSplit(Array(0.8, 0.2))

// Initialize ALS Model (using the old MLlib API in Spark 1.6.0)
val als = new ALS()
  .setRank(10)         // Number of latent factors
  .setIterations(10)   // Number of iterations
  .setLambda(0.1)      // Regularization parameter
  .setAlpha(1.0)       // Confidence parameter
  .setSeed(42)         // Random seed for reproducibility

// Train the ALS Model
val model = als.run(trainingData)

// Make Predictions
val predictions = model.predict(testData.map(rating => (rating.user, rating.product)))
  .map { case Rating(user, product, rating) => ((user, product), rating) }
  .join(testData.map { case Rating(user, product, rating) => ((user, product), rating) })
  .map { case ((user, product), (predictedRating, actualRating)) => (actualRating, predictedRating) }

// Evaluate the Model using RMSE (Root Mean Squared Error)
val metrics = new RegressionMetrics(predictions)
val rmse = metrics.rootMeanSquaredError
println(s"Root-mean-square error = $rmse")

// Recommend Top Movies for Each User (5 recommendations per user)
val userRecs = model.recommendProductsForUsers(5)
userRecs.collect().foreach { case (user, recs) =>
  println(s"User: $user")
  recs.foreach { case Rating(product, _, rating) =>
    println(s"  Movie: $product, Predicted Rating: $rating")
  }
}

// Recommend Top Users for Each Movie (5 recommendations per movie)
val movieRecs = model.recommendUsersForProducts(5)
movieRecs.collect().foreach { case (movie, recs) =>
  println(s"Movie: $movie")
  recs.foreach { case Rating(user, _, rating) =>
    println(s"  User: $user, Predicted Rating: $rating")
  }
}

//____________________________
//Week 12
// Import Libraries
import org.apache.spark.ml.fpm.FPGrowth
import org.apache.spark.sql.SparkSession

// Create Spark Session
val spark = SparkSession.builder().appName("FP-Growth").getOrCreate()

// Load Data
val data = spark.createDataFrame(Seq(
  (0, Array("apple", "banana", "cherry")),
  (1, Array("apple", "cherry")),
  (2, Array("banana", "cherry")),
  (3, Array("apple", "banana", "cherry", "date")),
  (4, Array("apple", "banana"))
)).toDF("id", "items")

// Initialize FP-Growth Model
val fpGrowth = new FPGrowth()
  .setItemsCol("items")
  .setMinSupport(0.2)   // Minimum support threshold
  .setMinConfidence(0.3) // Minimum confidence threshold

// Train the Model
val model = fpGrowth.fit(data)

// Display Frequent Itemsets
println("Frequent Itemsets:")
model.freqItemsets.show(false)

// Display Association Rules
println("Association Rules:")
model.associationRules.show(false)

// Display Predictions (Optional)
println("Predictions for Transactions:")
val predictions = model.transform(data)
predictions.show(false)



